{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "218b2994",
   "metadata": {},
   "source": [
    "# JEE Math Model: Environment & GPU Smoke Test\n",
    "\n",
    "This notebook verifies your Windows + 16GB GPU setup, installs required libraries, and runs small GPU smoke tests with Qwen2.5-Math-1.5B-Instruct and Aryabhata-1.0.\n",
    "\n",
    "What it does:\n",
    "- System and Python environment checks\n",
    "- Package versions and network checks\n",
    "- GPU readiness (torch.cuda, nvidia-smi)\n",
    "- Optional auth: Hugging Face and Weights & Biases\n",
    "- GPU smoke tests for Qwen2.5-Math-1.5B-Instruct and Aryabhata-1.0\n",
    "- Optional W&B logging of basic metrics\n",
    "\n",
    "Notes:\n",
    "- All cells are idempotent and will skip gracefully if tools or GPUs are missing.\n",
    "- On Windows, bitsandbytes 4-bit often isn’t supported; this notebook avoids 4-bit on Windows.\n",
    "- For best training performance consider Linux for vLLM/quant.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3850244",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'base (Python 3.13.2)' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages."
     ]
    }
   ],
   "source": [
    "# Section 1: Import Required Libraries (with safe fallbacks)\n",
    "import os, sys, platform, pathlib, subprocess, socket, ssl, json, hashlib, shutil, time\n",
    "\n",
    "# Optional libraries – import defensively\n",
    "try:\n",
    "    import requests  # type: ignore\n",
    "except Exception:\n",
    "    requests = None\n",
    "\n",
    "try:\n",
    "    import pandas as pd  # type: ignore\n",
    "except Exception:\n",
    "    pd = None\n",
    "\n",
    "try:\n",
    "    import psutil  # type: ignore\n",
    "except Exception:\n",
    "    psutil = None\n",
    "\n",
    "try:\n",
    "    import torch  # type: ignore\n",
    "except Exception:\n",
    "    torch = None\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf  # type: ignore\n",
    "except Exception:\n",
    "    tf = None\n",
    "\n",
    "from pathlib import Path\n",
    "print(\"[INFO] Imports complete. Python:\", sys.version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c607f535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 2: Python and OS Info Check\n",
    "print(\"[INFO] Python executable:\", sys.executable)\n",
    "print(\"[INFO] CWD:\", os.getcwd())\n",
    "print(\"[INFO] Platform:\", platform.platform())\n",
    "print(\"[INFO] System:\", platform.system(), platform.release(), platform.version())\n",
    "print(\"[INFO] Machine:\", platform.machine(), \"Processor:\", platform.processor())\n",
    "print(\"[INFO] sys.path head:\")\n",
    "for p in sys.path[:5]:\n",
    "    print(\"  \", p)\n",
    "\n",
    "# Kernel and locale info (best-effort)\n",
    "try:\n",
    "    import locale\n",
    "    print(\"[INFO] Locale:\", locale.getdefaultlocale())\n",
    "except Exception as e:\n",
    "    print(\"[WARN] Locale check failed:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc70dce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 3: Package Versions Check\n",
    "packages = [\n",
    "    (\"numpy\", \"np\"),\n",
    "    (\"pandas\", \"pd\"),\n",
    "    (\"requests\", \"requests\"),\n",
    "    (\"matplotlib\", \"matplotlib\"),\n",
    "    (\"scikit-learn\", \"sklearn\"),\n",
    "    (\"torch\", \"torch\"),\n",
    "    (\"tensorflow\", \"tensorflow\"),\n",
    "]\n",
    "\n",
    "versions = {}\n",
    "for pkg, mod_name in packages:\n",
    "    try:\n",
    "        mod = __import__(mod_name)\n",
    "        versions[pkg] = getattr(mod, \"__version__\", \"unknown\")\n",
    "    except Exception as e:\n",
    "        versions[pkg] = f\"not installed ({e})\"\n",
    "\n",
    "print(json.dumps(versions, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fece23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 4: Environment Variables and Paths Check\n",
    "interesting_env = [\"PATH\", \"PYTHONPATH\", \"HF_HOME\", \"TRANSFORMERS_CACHE\", \"WANDB_API_KEY\", \"HF_TOKEN\"]\n",
    "for k in interesting_env:\n",
    "    v = os.environ.get(k)\n",
    "    if not v:\n",
    "        print(f\"[ENV] {k}: <not set>\")\n",
    "    else:\n",
    "        display_val = v if k not in {\"WANDB_API_KEY\", \"HF_TOKEN\"} else (\"***\" if len(v) > 0 else \"\")\n",
    "        print(f\"[ENV] {k}: {display_val}\")\n",
    "\n",
    "# Paths validation\n",
    "paths_to_check = [\n",
    "    Path.cwd(),\n",
    "    Path.home(),\n",
    "    Path(os.environ.get(\"HF_HOME\", Path.home() / \".cache\" / \"huggingface\")),\n",
    "]\n",
    "\n",
    "for p in paths_to_check:\n",
    "    try:\n",
    "        exists = p.exists()\n",
    "        readable = os.access(p, os.R_OK)\n",
    "        writable = os.access(p, os.W_OK)\n",
    "        print(f\"[PATH] {p} exists={exists} read={readable} write={writable}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Path check failed for {p}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918f4830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 5: Network Connectivity Check\n",
    "host = \"example.com\"\n",
    "start = time.time()\n",
    "try:\n",
    "    ip = socket.gethostbyname(host)\n",
    "    print(f\"[NET] DNS resolved {host} -> {ip}\")\n",
    "except Exception as e:\n",
    "    print(f\"[NET] DNS resolution failed for {host}: {e}\")\n",
    "\n",
    "url = \"https://example.com\"\n",
    "resp_ok = False\n",
    "lat_ms = None\n",
    "try:\n",
    "    if requests is not None:\n",
    "        r = requests.get(url, timeout=5)\n",
    "        lat_ms = (time.time() - start) * 1000\n",
    "        print(f\"[NET] GET {url} status={r.status_code} latency_ms={lat_ms:.1f}\")\n",
    "        resp_ok = r.status_code == 200\n",
    "    else:\n",
    "        import urllib.request\n",
    "        with urllib.request.urlopen(url, timeout=5) as f:\n",
    "            lat_ms = (time.time() - start) * 1000\n",
    "            print(f\"[NET] GET {url} status={f.status} latency_ms={lat_ms:.1f}\")\n",
    "            resp_ok = f.status == 200\n",
    "except Exception as e:\n",
    "    print(f\"[NET] HTTP GET failed: {e}\")\n",
    "\n",
    "print(\"[NET] SSL available:\", hasattr(ssl, \"SSLContext\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7007eccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 6: Hardware Capability Check\n",
    "# CPU and memory\n",
    "try:\n",
    "    cpu_count = os.cpu_count()\n",
    "    print(\"[HW] CPU count:\", cpu_count)\n",
    "    if psutil:\n",
    "        vm = psutil.virtual_memory()\n",
    "        print(f\"[HW] RAM total={vm.total/1e9:.2f} GB, available={vm.available/1e9:.2f} GB\")\n",
    "except Exception as e:\n",
    "    print(\"[WARN] CPU/RAM check failed:\", e)\n",
    "\n",
    "# GPU via torch\n",
    "if torch is not None:\n",
    "    try:\n",
    "        print(\"[HW] torch version:\", torch.__version__)\n",
    "        print(\"[HW] CUDA available:\", torch.cuda.is_available())\n",
    "        if torch.cuda.is_available():\n",
    "            try:\n",
    "                dev_name = torch.cuda.get_device_name(0)\n",
    "            except Exception:\n",
    "                dev_name = \"<unknown>\"\n",
    "            print(\"[HW] CUDA device 0:\", dev_name)\n",
    "            try:\n",
    "                print(\"[HW] CUDA capability:\", torch.cuda.get_device_capability(0))\n",
    "            except Exception:\n",
    "                pass\n",
    "    except Exception as e:\n",
    "        print(\"[WARN] Torch GPU check failed:\", e)\n",
    "else:\n",
    "    print(\"[HW] torch not installed; skipping torch GPU check\")\n",
    "\n",
    "# GPU via TensorFlow\n",
    "if tf is not None:\n",
    "    try:\n",
    "        gpus = tf.config.list_physical_devices('GPU')\n",
    "        print(\"[HW] TensorFlow GPUs:\", gpus)\n",
    "    except Exception as e:\n",
    "        print(\"[WARN] TensorFlow GPU check failed:\", e)\n",
    "else:\n",
    "    print(\"[HW] tensorflow not installed; skipping TF GPU check\")\n",
    "\n",
    "# nvidia-smi\n",
    "try:\n",
    "    result = subprocess.run([\"nvidia-smi\", \"--query-gpu=name,memory.total\", \"--format=csv,noheader\"],\n",
    "                            capture_output=True, text=True, timeout=5)\n",
    "    if result.returncode == 0:\n",
    "        print(\"[HW] nvidia-smi output:\")\n",
    "        print(result.stdout)\n",
    "    else:\n",
    "        print(\"[HW] nvidia-smi not available or error.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"[HW] nvidia-smi not found (likely no NVIDIA drivers or PATH not set)\")\n",
    "except Exception as e:\n",
    "    print(\"[WARN] nvidia-smi check failed:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad62fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 7: Data Sanity Checks with pandas\n",
    "CSV_PATH = os.environ.get(\"JEE_SAMPLE_CSV\", \"\")\n",
    "\n",
    "if pd is None:\n",
    "    print(\"[DATA] pandas not installed; creating a small fallback structure\")\n",
    "    sample = [{\"x\": i, \"y\": i * 2} for i in range(5)]\n",
    "    print(sample)\n",
    "else:\n",
    "    if CSV_PATH and Path(CSV_PATH).exists():\n",
    "        try:\n",
    "            df = pd.read_csv(CSV_PATH)\n",
    "            print(\"[DATA] Loaded:\", CSV_PATH)\n",
    "        except Exception as e:\n",
    "            print(\"[DATA] Failed to load CSV:\", e)\n",
    "            df = pd.DataFrame({\"x\": range(5), \"y\": [i * 2 for i in range(5)]})\n",
    "    else:\n",
    "        df = pd.DataFrame({\"x\": range(5), \"y\": [i * 2 for i in range(5)]})\n",
    "        print(\"[DATA] Using sample DataFrame\")\n",
    "\n",
    "    try:\n",
    "        print(\"[DATA] shape:\", df.shape)\n",
    "        print(\"[DATA] dtypes:\\n\", df.dtypes)\n",
    "        print(\"[DATA] head:\\n\", df.head())\n",
    "        print(\"[DATA] describe:\\n\", df.describe(include='all'))\n",
    "        print(\"[DATA] missing values:\\n\", df.isna().sum())\n",
    "        # Simple schema validation\n",
    "        assert df.shape[0] > 0, \"DataFrame must not be empty\"\n",
    "        assert df.shape[1] > 0, \"DataFrame must have columns\"\n",
    "        print(\"[DATA] Basic schema validation passed\")\n",
    "    except Exception as e:\n",
    "        print(\"[WARN] Data checks failed:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636dffc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 8: Quick Unit Checks and Summary Report\n",
    "summary = {\n",
    "    \"python\": sys.version,\n",
    "    \"platform\": platform.platform(),\n",
    "    \"cuda_available\": bool(torch and torch.cuda.is_available()) if torch else False,\n",
    "    \"requests\": versions.get(\"requests\"),\n",
    "}\n",
    "\n",
    "# Quick assertions (best-effort)\n",
    "try:\n",
    "    assert isinstance(summary[\"python\"], str)\n",
    "    print(\"[CHECK] Python version string OK\")\n",
    "except AssertionError:\n",
    "    print(\"[CHECK] Python version string FAILED\")\n",
    "\n",
    "try:\n",
    "    # If requests installed, expect a plausible version or unknown\n",
    "    if isinstance(summary.get(\"requests\"), str):\n",
    "        print(\"[CHECK] requests version present\")\n",
    "except Exception:\n",
    "    print(\"[CHECK] requests version check skipped\")\n",
    "\n",
    "print(\"[SUMMARY]\\n\" + json.dumps(summary, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8692c04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install cell(s): Upgrade pip and install core libs\n",
    "import sys, subprocess\n",
    "\n",
    "def pip_install(pkgs):\n",
    "    try:\n",
    "        print(\"[PIP] Installing:\", pkgs)\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\"] + pkgs)\n",
    "        print(\"[PIP] Done\")\n",
    "    except Exception as e:\n",
    "        print(\"[PIP] Failed:\", e)\n",
    "\n",
    "# Upgrade basics\n",
    "pip_install([\"pip\", \"setuptools\", \"wheel\"])  # safe re-run\n",
    "\n",
    "# Core libs (torch install string may require manual edit per CUDA)\n",
    "# For Windows CUDA 12.x, user may prefer pip torch index URL; here we try default as a start\n",
    "core = [\n",
    "    \"transformers>=4.43.0\",\n",
    "    \"accelerate>=0.30.0\",\n",
    "    \"safetensors\",\n",
    "    \"sentencepiece\",\n",
    "    \"einops\",\n",
    "    \"wandb\",\n",
    "    \"datasets\",\n",
    "    \"tiktoken\",\n",
    "]\n",
    "\n",
    "# Try torch separately to give clearer errors\n",
    "try:\n",
    "    import torch  # noqa: F401\n",
    "    print(\"[PIP] torch already installed:\", torch.__version__)\n",
    "except Exception:\n",
    "    print(\"[PIP] torch not found; attempting install (CPU-only fallback if CUDA fails)\")\n",
    "    try:\n",
    "        pip_install([\"torch\", \"torchvision\", \"torchaudio\"])  # may install CPU build\n",
    "    except Exception as e:\n",
    "        print(\"[PIP] torch install attempt failed:\", e)\n",
    "\n",
    "# Install the rest\n",
    "pip_install(core)\n",
    "\n",
    "print(\"[INSTALL] Completed. You may need to restart kernel if torch was newly installed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8dd488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Hugging Face Auth\n",
    "hf_token = os.environ.get(\"HF_TOKEN\") or os.environ.get(\"HUGGINGFACE_TOKEN\")\n",
    "try:\n",
    "    if hf_token:\n",
    "        from huggingface_hub import login\n",
    "        login(token=hf_token)\n",
    "        print(\"[HF] Logged in via token from env.\")\n",
    "    else:\n",
    "        print(\"[HF] No token found in env; public models should still work.\")\n",
    "except Exception as e:\n",
    "    print(\"[HF] Login skipped or failed:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1762627e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Weights & Biases Auth\n",
    "try:\n",
    "    import wandb\n",
    "    wandb_key = os.environ.get(\"WANDB_API_KEY\")\n",
    "    if wandb_key:\n",
    "        try:\n",
    "            wandb.login(key=wandb_key)\n",
    "            print(\"[W&B] Logged in via env key.\")\n",
    "        except Exception as e:\n",
    "            print(\"[W&B] Login failed:\", e)\n",
    "    else:\n",
    "        print(\"[W&B] WANDB_API_KEY not set; skipping login.\")\n",
    "except Exception as e:\n",
    "    print(\"[W&B] wandb not installed or login skipped:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1e51b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qwen2.5-Math-1.5B-Instruct GPU Smoke Test\n",
    "import time\n",
    "\n",
    "qwen_model_id = \"Qwen/Qwen2.5-Math-1.5B-Instruct\"\n",
    "\n",
    "try:\n",
    "    from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "    if torch is None:\n",
    "        raise RuntimeError(\"torch not installed\")\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"[QWEN] CUDA not available; using CPU. This will be slow.\")\n",
    "\n",
    "    print(f\"[QWEN] Loading {qwen_model_id} ...\")\n",
    "    t0 = time.time()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(qwen_model_id)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        qwen_model_id,\n",
    "        torch_dtype=\"auto\",\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    t_load = time.time() - t0\n",
    "    print(f\"[QWEN] Loaded in {t_load:.1f}s\")\n",
    "\n",
    "    prompt = \"Solve step by step: If 2x + 3 = 11, what is x?\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "    t1 = time.time()\n",
    "    out = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=64,\n",
    "        do_sample=False,\n",
    "        temperature=0.0,\n",
    "    )\n",
    "    t_gen = time.time() - t1\n",
    "\n",
    "    text = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "    print(\"[QWEN] Output:\\n\", text)\n",
    "    print(f\"[QWEN] Generation time: {t_gen:.2f}s\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"[QWEN] Peak VRAM: {torch.cuda.max_memory_allocated()/1e9:.2f} GB\")\n",
    "except Exception as e:\n",
    "    print(\"[QWEN] Smoke test failed:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33a910e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aryabhata-1.0 GPU Smoke Test\n",
    "ary_model_id = \"PhysicsWallahAI/Aryabhata-1.0\"\n",
    "\n",
    "try:\n",
    "    from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "    if torch is None:\n",
    "        raise RuntimeError(\"torch not installed\")\n",
    "\n",
    "    print(f\"[ARY] Loading {ary_model_id} ...\")\n",
    "    t0 = time.time()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(ary_model_id)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        ary_model_id,\n",
    "        torch_dtype=\"auto\",\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    print(f\"[ARY] Loaded in {time.time()-t0:.1f}s\")\n",
    "\n",
    "    prompt = \"You are a helpful math tutor. Solve step by step: Evaluate 3*(4+5).\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "    t1 = time.time()\n",
    "    out = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=64,\n",
    "        do_sample=False,\n",
    "        temperature=0.0,\n",
    "    )\n",
    "    t_gen = time.time() - t1\n",
    "\n",
    "    text = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "    print(\"[ARY] Output:\\n\", text)\n",
    "    print(f\"[ARY] Generation time: {t_gen:.2f}s\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"[ARY] Peak VRAM: {torch.cuda.max_memory_allocated()/1e9:.2f} GB\")\n",
    "except RuntimeError as e:\n",
    "    msg = str(e)\n",
    "    if \"CUDA out of memory\" in msg:\n",
    "        print(\"[ARY][OOM] Out of VRAM. Tips: reduce max_new_tokens, set torch_dtype=torch.float16, or try CPU to validate.\")\n",
    "        print(\"[ARY][NOTE] 4-bit quant (bitsandbytes) is not recommended on Windows; prefer Linux for 4-bit.\")\n",
    "    else:\n",
    "        print(\"[ARY] RuntimeError:\", e)\n",
    "except Exception as e:\n",
    "    print(\"[ARY] Smoke test failed:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4e8fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Log simple metrics to W&B\n",
    "try:\n",
    "    import wandb\n",
    "    run = None\n",
    "    if os.environ.get(\"WANDB_API_KEY\"):\n",
    "        try:\n",
    "            wandb.login(key=os.environ[\"WANDB_API_KEY\"])\n",
    "            run = wandb.init(project=\"jee-math-smoke\", name=\"env_gpu_smoke\", reinit=True)\n",
    "            metrics = {\n",
    "                \"system/platform\": platform.platform(),\n",
    "                \"torch/cuda_available\": bool(torch and torch.cuda.is_available()),\n",
    "            }\n",
    "            wandb.log(metrics)\n",
    "            print(\"[W&B] Logged metrics:\", metrics)\n",
    "        except Exception as e:\n",
    "            print(\"[W&B] Logging failed:\", e)\n",
    "    else:\n",
    "        print(\"[W&B] Skipping logging; WANDB_API_KEY not set.\")\n",
    "    if run is not None:\n",
    "        run.finish()\n",
    "except Exception as e:\n",
    "    print(\"[W&B] wandb not installed or logging skipped:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168c350d",
   "metadata": {},
   "source": [
    "## Next steps and tips\n",
    "\n",
    "- If torch installed CPU-only, install CUDA build that matches your NVIDIA drivers.\n",
    "- Consider Linux for vLLM and 4-bit quantization workflows.\n",
    "- For larger context windows and faster inference, explore vLLM or TensorRT-LLM (Linux recommended).\n",
    "- Build your dataset pipeline next (PDF parsing, scraping, dedup, quality filters) per tasks.\n",
    "\n",
    "Model cards:\n",
    "- Qwen2.5-Math-1.5B-Instruct: https://huggingface.co/Qwen/Qwen2.5-Math-1.5B-Instruct\n",
    "- Aryabhata-1.0: https://huggingface.co/PhysicsWallahAI/Aryabhata-1.0\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
