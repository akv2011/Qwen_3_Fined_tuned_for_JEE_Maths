# JEE Mathematics Fine-tuning Project

Fine-tuning Qwen3-7B-Instruct on JEE (Joint Entrance Examination) Mathematics problems using Unsloth for optimized training.

## Project Structure

```
Qwen_3_Fined_tuned_for_JEE_Maths/
â”œâ”€â”€ data/                          # Raw data and databases
â”‚   â”œâ”€â”€ raw/                       # Original PDF files
â”‚   â”‚   â””â”€â”€ 41 Years IIT JEE Mathematics by Amit M Agarwal.pdf
â”‚   â””â”€â”€ demo_data/                 # Demo database and samples
â”‚       â”œâ”€â”€ jee_problems.db        # SQLite database (3 samples)
â”‚       â”œâ”€â”€ processed/
â”‚       â”œâ”€â”€ raw_pdfs/
â”‚       â”œâ”€â”€ statistics/
â”‚       â””â”€â”€ structured/
â”‚
â”œâ”€â”€ datasets/                      # Processed datasets
â”‚   â””â”€â”€ jee_math/                  # HuggingFace format dataset
â”‚       â”œâ”€â”€ dataset.json           # 625 JEE problems
â”‚       â”œâ”€â”€ dataset_dict.json
â”‚       â”œâ”€â”€ train/
â”‚       â””â”€â”€ test/
â”‚
â”œâ”€â”€ notebooks/                     # Jupyter notebooks
â”‚   â”œâ”€â”€ fine_tune_qwen_with_unsloth.ipynb  # Main training notebook
â”‚   â”œâ”€â”€ jee_math_env_and_smoke.ipynb
â”‚   â””â”€â”€ kaggle_jee_processing_pipeline.ipynb
â”‚
â”œâ”€â”€ scripts/                       # Python scripts
â”‚   â”œâ”€â”€ analyze_dataset.py
â”‚   â”œâ”€â”€ analyze_extraction.py
â”‚   â”œâ”€â”€ create_huggingface_dataset.py
â”‚   â”œâ”€â”€ create_jee_hf_dataset.py
â”‚   â”œâ”€â”€ create_local_dataset.py
â”‚   â”œâ”€â”€ convert_to_huggingface_format.py
â”‚   â”œâ”€â”€ demo_local_dataset.py
â”‚   â”œâ”€â”€ examine_data.py
â”‚   â”œâ”€â”€ examine_llamaparse.py
â”‚   â”œâ”€â”€ extraction_results_table.py
â”‚   â”œâ”€â”€ jee_extraction_comparison.py
â”‚   â”œâ”€â”€ jee_extraction_demo.py
â”‚   â”œâ”€â”€ pdf_deep_analysis.py
â”‚   â”œâ”€â”€ setup_extraction_comparison.py
â”‚   â”œâ”€â”€ smoke_test.py
â”‚   â”œâ”€â”€ test_extraction.py
â”‚   â””â”€â”€ test_langextract.py
â”‚
â”œâ”€â”€ results/                       # Analysis results
â”‚   â”œâ”€â”€ extraction_results/        # PDF extraction outputs
â”‚   â”œâ”€â”€ jee_sample_extraction.json
â”‚   â””â”€â”€ pdf_analysis_report.json
â”‚
â”œâ”€â”€ docs/                          # Documentation
â”‚   â”œâ”€â”€ CLAUDE.md
â”‚   â”œâ”€â”€ UPDATED_PLAN.md
â”‚   â”œâ”€â”€ pdf_analysis_summary.md
â”‚   â”œâ”€â”€ jee_sample_summary.txt
â”‚   â”œâ”€â”€ requirements_comparison.txt
â”‚   â””â”€â”€ requirements_local.txt
â”‚
â”œâ”€â”€ terminal_outputs/              # Terminal logs
â”‚
â”œâ”€â”€ .env.example                   # Environment variables template
â”œâ”€â”€ requirements.txt               # Python dependencies
â””â”€â”€ README.md                      # This file
```

## Quick Start

### 1. Installation

```bash
# Clone the repository
git clone <repository-url>
cd Qwen_3_Fined_tuned_for_JEE_Maths

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### 2. Setup Environment

```bash
# Copy environment template
cp .env.example .env

# Add your API keys to .env
# ANTHROPIC_API_KEY=your_key_here
# HUGGINGFACE_TOKEN=your_token_here
```

### 3. Run Fine-tuning

Open and run the main notebook:
```bash
jupyter notebook notebooks/fine_tune_qwen_with_unsloth.ipynb
```

## Dataset

- **Source**: 41 Years IIT JEE Mathematics by Amit M Agarwal
- **Size**: 625 problems extracted from PDF
- **Format**: Conversational format with problem/solution pairs
- **Topics**: Calculus, Algebra, Trigonometry, Geometry, etc.

## Model Details

- **Base Model**: Qwen 3-7B-Instruct (7.6B parameters)
- **Fine-tuning Method**: LoRA (Low-Rank Adaptation)
- **Quantization**: 4-bit for memory efficiency
- **Framework**: Unsloth (2x faster fine-tuning)
- **Trainable Parameters**: ~40M (0.53% of total)

## Training Configuration

- **Batch Size**: 2 (effective: 8 with gradient accumulation)
- **Learning Rate**: 2e-4
- **Optimizer**: AdamW 8-bit
- **Max Steps**: 60
- **LoRA Rank**: 16
- **Target Modules**: q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj

## Training Results

- **Final Loss**: 0.2250
- **Training Time**: ~9.34 minutes (560 seconds)
- **Hardware**: Tesla P100-PCIE-16GB GPU
- **Steps/Second**: 0.11

## Usage Example

```python
from unsloth import FastLanguageModel
import torch

# Load fine-tuned model
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="outputs/final_model",
    max_seq_length=2048,
    load_in_4bit=True,
)

FastLanguageModel.for_inference(model)

# Test with a problem
problem = "Find the derivative of f(x) = xÂ³ + 2xÂ² - 5x + 3"
prompt = f"""Below is a mathematics problem. Solve it step by step.

### Problem:
{problem}

### Solution:"""

inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
outputs = model.generate(**inputs, max_new_tokens=300)
solution = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(solution)
```

## ğŸ› ï¸ Scripts Overview

### Data Processing
- `create_local_dataset.py` - Create local SQLite database
- `create_jee_hf_dataset.py` - Convert to HuggingFace format
- `convert_to_huggingface_format.py` - Format conversion utilities

### Analysis
- `analyze_dataset.py` - Dataset statistics and analysis
- `analyze_extraction.py` - PDF extraction quality analysis
- `pdf_deep_analysis.py` - Deep PDF structure analysis

### Extraction Comparison
- `jee_extraction_comparison.py` - Compare extraction methods
- `extraction_results_table.py` - Tabular comparison results

### Testing
- `smoke_test.py` - Quick functionality tests
- `test_extraction.py` - Test extraction pipeline
- `demo_local_dataset.py` - Demo dataset usage

## Key Notebooks

1. **fine_tune_qwen_with_unsloth.ipynb** - Main training pipeline
   - Data preparation (10 synthetic problems + 625 real problems)
   - Model loading and configuration
   - LoRA fine-tuning
   - Model testing and evaluation

2. **jee_math_env_and_smoke.ipynb** - Environment setup and smoke tests

3. **kaggle_jee_processing_pipeline.ipynb** - Full processing pipeline

## Next Steps

- [ ] Evaluate on full validation set
- [ ] Compare fine-tuned vs base model performance
- [ ] Upload to HuggingFace Hub
- [ ] Test on more complex JEE problems
- [ ] Create inference API
- [ ] Build web interface for problem solving

